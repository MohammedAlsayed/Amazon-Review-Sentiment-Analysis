{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Amazon Review Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Amazon reviews for Sentiment Analysis\n",
        "Data source: \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\"\n",
        "\n",
        "The following has been conducted in the notebook:\n",
        "\n",
        "1- Data Preprocessing\n",
        "\n",
        "2- Transforming text into word2vector using google word2vec, and TF-IDF\n",
        "\n",
        "3- Applying different machine learning models to predict positive, negative sentiment. \n",
        "\n",
        "**Best accuracy achieved 0.89 f1 score, with SVM TF-IDF**"
      ],
      "metadata": {
        "id": "tlbaP4yMqq0k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8sMmNycLN5c",
        "outputId": "3bacb1a0-9a67-4382-c84d-a21721aa4f6f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\"\n",
        "data_compressed = requests.get(url).content\n",
        "data = pd.read_csv(BytesIO(data_compressed), compression='gzip', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZgGW0HEqU38"
      },
      "source": [
        "data = pd.read_csv(BytesIO(data_compressed), compression='gzip', sep='\\t', error_bad_lines=False, warn_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubSZ2Oi4pIx7"
      },
      "source": [
        "def review_num_words(txt):\n",
        "    return len(txt.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ijfbTvESLdN"
      },
      "source": [
        "data = data.dropna().reset_index(drop=True)\n",
        "# reviews with more than 20 words\n",
        "data['review_length'] = data['review_body'].apply(lambda body : review_num_words(body))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O60iRldyue7n"
      },
      "source": [
        "data_df = data.loc[data['review_length'] > 54]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei616t2yNg3g"
      },
      "source": [
        "# Filter out 150K reviews \n",
        "positive = data_df.loc[data['star_rating'] > 3].sample(n=50000, replace=False)\n",
        "negative = data_df.loc[data['star_rating'] < 3].sample(n=50000, replace=False)\n",
        "neutral = data_df.loc[data['star_rating'] == 3].sample(n=50000, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKhBvlQ6XFXu"
      },
      "source": [
        "reviews = positive.append(negative)\n",
        "reviews = reviews.append(neutral)\n",
        "reviews = reviews[['review_body', 'star_rating']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnUtMtUm4vqz",
        "outputId": "d51d964e-c2ae-4dfd-c041-c0bd534ffc9b"
      },
      "source": [
        "reviews.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review_body    150000\n",
              "star_rating    150000\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpnHj_W-Yc_G"
      },
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYpWNlMhXrH-",
        "outputId": "ccbb9af5-524c-408e-a192-f1ae2cf63a82"
      },
      "source": [
        "# Remove HTML tags funtion\n",
        "def remove_tags(txt):\n",
        "    # parse html content\n",
        "    soup = BeautifulSoup(txt, \"html.parser\")\n",
        "\n",
        "    # get tags content\n",
        "    for data in soup(['style', 'script']):\n",
        "        data.get_text()\n",
        "\n",
        "    # return html's tag content\n",
        "    return ' '.join(soup.stripped_strings)\n",
        "    \n",
        "# Remove URLS funtion\n",
        "import re\n",
        "def remove_urls(txt):\n",
        "  return re.sub(r\"http\\S+\", \"\", txt)\n",
        "\n",
        "!pip install contractions\n",
        "\n",
        "# Apply contraction to words\n",
        "import contractions\n",
        "def contractionfunction(s):\n",
        "  expanded_words = []\n",
        "  for word in s.split():\n",
        "    expanded_words.append(contractions.fix(word))\n",
        "\n",
        "  result = ' '.join(expanded_words)   \n",
        "  return result\n",
        "\n",
        "\n",
        "def remove_non_alphabetical(txt):\n",
        "    regex = re.compile('[\\W_0-9]+')\n",
        "    dirty_list = txt.split()\n",
        "    clean_list = [regex.sub(' ', word) for word in dirty_list]\n",
        "    clean_string = ' '.join(clean_list)\n",
        "    return clean_string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# remove stop words function\n",
        "def remove_stop_words(txt):\n",
        "    stop = stopwords.words('english')\n",
        "    word_list = txt.split()\n",
        "    clean_list = []\n",
        "    clean_string = ''\n",
        "    for word in word_list:\n",
        "      if word not in stop:\n",
        "        clean_list.append(word)\n",
        "    clean_string = ' '.join(clean_list)\n",
        "    return clean_string\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def leammatize_review(txt):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_list = txt.split()\n",
        "  clean_list = []\n",
        "  clean_string = ''\n",
        "  for word in word_list:\n",
        "    new_word = lemmatizer.lemmatize(word)\n",
        "    clean_list.append(new_word)\n",
        "  clean_string = ' '.join(clean_list)\n",
        "  return clean_string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlhcabH_W-bF"
      },
      "source": [
        "reviews['clean_body'] = reviews['review_body'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e4wwrm8YfVf"
      },
      "source": [
        "# Applying preproccessing\n",
        "reviews['clean_body'] = reviews['clean_body'].apply(lambda body : remove_tags(body))\n",
        "reviews['clean_body'] = reviews['clean_body'].apply(lambda body : remove_urls(body))\n",
        "reviews['clean_body'] = reviews['clean_body'].apply(lambda body : contractionfunction(body))\n",
        "reviews['clean_body'] = reviews['clean_body'].apply(lambda body : remove_non_alphabetical(body))\n",
        "reviews['clean_body'] = reviews['clean_body'].apply(lambda review: remove_stop_words(review))\n",
        "reviews['clean_body'] = reviews['clean_body'].apply(lambda txt: leammatize_review(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6xfaha6f_KY"
      },
      "source": [
        "# Classifying our project to positive and negative\n",
        "reviews.loc[reviews['star_rating'] > 3, 'binary_rate'] = 1\n",
        "reviews.loc[reviews['star_rating'] < 3, 'binary_rate'] = 0\n",
        "\n",
        "reviews.loc[reviews['star_rating'] > 3, 'trinary_rate'] = 2\n",
        "reviews.loc[reviews['star_rating'] == 3, 'trinary_rate'] = 1\n",
        "reviews.loc[reviews['star_rating'] < 3, 'trinary_rate'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhYmCmuLQqdT"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Rx6s8Y14rY"
      },
      "source": [
        "## Section (a): Google Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXpL0lXVOPbp",
        "outputId": "f7f9506f-8b85-4ead-d1fd-2babed277eaa"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkmjqdCnPdMy",
        "outputId": "8cf1ab58-133f-4597-d3c1-a56a5118b740"
      },
      "source": [
        "wv.most_similar(positive=['america', 'london'], negative=['washington'], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('england', 0.569724440574646)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HERLom1qOWOb",
        "outputId": "2a0fdbca-5c27-429f-f7c0-2952a27c13bf"
      },
      "source": [
        "wv.most_similar(positive=['kitchen', 'farm'], negative=['chef'], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('dairy_farm', 0.5582115650177002)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJMR2ZwPnRC"
      },
      "source": [
        "## Section(b): Training my Own Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZU_l9hKX9J1"
      },
      "source": [
        "# Create list of words for every review\n",
        "sent = [row.split() for row in reviews['clean_body']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3sQBXAKRJU3"
      },
      "source": [
        "# Build the model\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(min_count=10,\n",
        "                     window=11,\n",
        "                     size=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-JQbJpAW-8U"
      },
      "source": [
        "# Find phrases in our dataset such as New York to be made as one word\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "phrases = Phrases(sent, min_count=10)\n",
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YIBggAklaDZ"
      },
      "source": [
        "# Add our words and pharses to the model \n",
        "w2v_model.build_vocab(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEvCj7xlUN4C",
        "outputId": "615182f5-0b98-491b-b53e-5e1be2bda418"
      },
      "source": [
        "# Train our model on our vocab\n",
        "from time import time\n",
        "t = time()\n",
        "\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=10)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 5.8 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSiM5MtBX0DR",
        "outputId": "78053c30-a325-468c-bdbe-1a621e8fdd38"
      },
      "source": [
        "# Find nearset vector\n",
        "w2v_model.most_similar(positive=['america', 'london'], negative=['washington'], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('germany', 0.41902074217796326)]"
            ]
          },
          "execution_count": 274,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY1iNfB4dpXs",
        "outputId": "5877f08f-c008-4681-9169-0c15e4047614"
      },
      "source": [
        "w2v_model.most_similar(positive=['kitchen', 'farm'], negative=['chef'], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('downstairs', 0.38579726219177246)]"
            ]
          },
          "execution_count": 275,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emKCN5qJfbVq"
      },
      "source": [
        "# Simple models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2gxdXSAfDdh",
        "outputId": "9e85de2e-05c6-429b-9145-f0908c8b2360"
      },
      "source": [
        "# Trained word2vec\n",
        "import numpy as np\n",
        "\n",
        "indexs = reviews.index\n",
        "reviews['word2vec'] = pd.Series(dtype=object)\n",
        "for idx , review in zip(indexs, reviews['clean_body']):\n",
        "  unseen_words = 0\n",
        "  n = len(review.split())\n",
        "  x = 0\n",
        "  for word in review.split():\n",
        "    try:\n",
        "      x = x + w2v_model[word]\n",
        "    except KeyError:\n",
        "         unseen_words = unseen_words + 1\n",
        "  if unseen_words == n:\n",
        "    reviews.at[idx, 'word2vec'] = np.NaN\n",
        "    continue\n",
        "  x = x/(n-unseen_words)\n",
        "  x1 = x.reshape(-1, 1)\n",
        "  x1 = x1.T\n",
        "  reviews.at[idx, 'word2vec'] = x1[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9V8eK4sswx7"
      },
      "source": [
        "# Google pre-trained word2vec\n",
        "import numpy as np\n",
        "\n",
        "indexs = reviews.index\n",
        "reviews['google_word2vec'] = pd.Series(dtype=object)\n",
        "for idx , review in zip(indexs, reviews['clean_body']):\n",
        "  unseen_words = 0\n",
        "  n = len(review.split())\n",
        "  x = 0\n",
        "  for word in review.split():\n",
        "    try:\n",
        "      x = x + wv[word]\n",
        "    except KeyError:\n",
        "         unseen_words = unseen_words + 1\n",
        "  if unseen_words == n:\n",
        "    reviews.at[idx, 'word2vec'] = np.NaN\n",
        "    continue\n",
        "  x = x/(n-unseen_words)\n",
        "  x1 = x.reshape(-1, 1)\n",
        "  x1 = x1.T\n",
        "  reviews.at[idx, 'google_word2vec'] = x1[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPvh-CLjla3p"
      },
      "source": [
        "### Dataset prepration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE-vN1qu5D0D"
      },
      "source": [
        "# Trained Word2Vec training and testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Drop the neutral reviews\n",
        "binary_reviews = reviews[['clean_body','word2vec', 'binary_rate']].dropna()\n",
        "# Create our X dataset\n",
        "X_word2vec = np.array(binary_reviews['word2vec'].values.tolist())\n",
        "y_word2vec = binary_reviews['binary_rate']\n",
        "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_word2vec, y_word2vec, test_size=0.2, random_state=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYEHEPrLl-FT"
      },
      "source": [
        "# TFIDF training and testing set\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X_tfidf = binary_reviews['clean_body']\n",
        "y_tfidf = binary_reviews['binary_rate']\n",
        "X_train, X_test, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=200)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-8Lv8ILl99y"
      },
      "source": [
        "# Pretrained Google word2vec training and testing set\n",
        "# Drop the neutral reviews\n",
        "google_binary_reviews = reviews[['clean_body','google_word2vec', 'binary_rate']].dropna()\n",
        "# Create our X dataset\n",
        "X_google_word2vec = np.array(google_binary_reviews['google_word2vec'].values.tolist())\n",
        "y_google_word2vec = google_binary_reviews['binary_rate']\n",
        "X_train_google_word2vec, X_test_google_word2vec, y_train_google_word2vec, y_test_google_word2vec = train_test_split(X_google_word2vec, y_google_word2vec, test_size=0.2, random_state=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhd0TqREAiYH"
      },
      "source": [
        "def report_print(training_report, testing_report):\n",
        "  # Training report of model\n",
        "  tr_accuracy = training_report['accuracy']\n",
        "  tr_precision = training_report['1.0']['precision']\n",
        "  tr_recall = training_report['1.0']['recall']\n",
        "  tr_f1_score = training_report['1.0']['f1-score']\n",
        "\n",
        "  # Testing report of model\n",
        "  te_accuracy = testing_report['accuracy']\n",
        "  te_precision = testing_report['1.0']['precision']\n",
        "  te_recall = testing_report['1.0']['recall']\n",
        "  te_f1_score = testing_report['1.0']['f1-score']\n",
        "  print(f'{tr_accuracy}, {tr_precision}, {tr_recall}, {tr_f1_score}, {te_accuracy}, {te_precision}, {te_recall}, {te_f1_score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deAO3zvqnXRp"
      },
      "source": [
        "### Perceptron trained word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2ScmjKoAHEc",
        "outputId": "f4f4f84c-21de-46ce-8358-044b7a3d2e95"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clfPercep = Perceptron()\n",
        "cv=StratifiedKFold(n_splits=5)\n",
        "parameters = {'penalty':('l2','l1') ,'alpha': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]}\n",
        "gridsearch = GridSearchCV(clfPercep, parameters,\n",
        "\t\t\t\t\t\t\t\t\t          cv=cv, scoring='f1')\n",
        "\n",
        "# Find the best params with grid search\n",
        "gridsearch.fit(X_train_word2vec, y_train_word2vec)\n",
        "print(\"Best params: {}\".format(gridsearch.best_params_))\n",
        "print(\"Best f1 score: %.5f\" % gridsearch.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: {'alpha': 0.0001, 'penalty': 'l1'}\n",
            "Best f1 score: 0.80147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QMZ5vQGba1r",
        "outputId": "41dede73-9cd6-4312-ae2c-81082a3320ac"
      },
      "source": [
        "# Train the perceptron\n",
        "clfPercep = Perceptron(alpha= 0.0001, penalty='l1')\n",
        "clfPercep.fit(X_train_word2vec, y_train_word2vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
              "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "           penalty='l1', random_state=0, shuffle=True, tol=0.001,\n",
              "           validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LvojORMAlaN",
        "outputId": "a7722c9a-b10e-4b06-e9f3-509c19a6be4b"
      },
      "source": [
        "print(\"Perceptron model report on training and testing data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec, clfPercep.predict(X_train_word2vec), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec, clfPercep.predict(X_test_word2vec), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron model report on training and testing data: \n",
            "\n",
            "0.84135, 0.831537565521258, 0.8563074770091963, 0.8437407662759775, 0.84275, 0.831379009593953, 0.8592748397435898, 0.8450967837265428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PZ9Zeven4Sh"
      },
      "source": [
        "### Perceptron TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbdN_LyqrofE",
        "outputId": "064c88de-3af5-4204-f124-8b872909b6a2"
      },
      "source": [
        "clfPercep = Perceptron()\n",
        "clfPercep.fit(X_train_tfidf, y_train_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
              "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
              "           validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c15URz_Srqki",
        "outputId": "78ba4399-eb08-4bb6-c9e7-e2fa226227ef"
      },
      "source": [
        "print(\"Perceptron model report on training and testing data: \\n\")\n",
        "training_report = classification_report(y_train_tfidf, clfPercep.predict(X_train_tfidf), output_dict=True)\n",
        "testing_report = classification_report(y_test_tfidf, clfPercep.predict(X_test_tfidf), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron model report on training and testing data: \n",
            "\n",
            "0.9461875, 0.9419787618505409, 0.9509946021591363, 0.9464652117142324, 0.85395, 0.8507996423959472, 0.8578725961538461, 0.8543214802254252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjtaAaMun8iG"
      },
      "source": [
        "### Perceptron Google word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvA49hsa1uhH",
        "outputId": "e8c28345-6d17-438e-d551-bb5a4d171ff7"
      },
      "source": [
        "clfPercep = Perceptron()\n",
        "cv=StratifiedKFold(n_splits=5)\n",
        "parameters = {'penalty':('l2','l1') ,'alpha': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]}\n",
        "gridsearch = GridSearchCV(clfPercep, parameters,\n",
        "\t\t\t\t\t\t\t\t\t          cv=cv, scoring='f1')\n",
        "\n",
        "# Find the best params with grid search\n",
        "gridsearch.fit(X_train_google_word2vec, y_train_google_word2vec)\n",
        "print(\"Best params: {}\".format(gridsearch.best_params_))\n",
        "print(\"Best f1 score: %.5f\" % gridsearch.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'alpha': 0.0003, 'penalty': 'l2'}\n",
            "Best f1 score: 0.79345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8jYnI1A18qz",
        "outputId": "b0844a47-6eda-4c49-bc60-0e1623a8896a"
      },
      "source": [
        "print(\"Perceptron model report on training and testing data: \\n\")\n",
        "training_report = classification_report(y_train_google_word2vec, gridsearch.predict(X_train_google_word2vec), output_dict=True)\n",
        "testing_report = classification_report(y_test_google_word2vec, gridsearch.predict(X_test_google_word2vec), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron model report on training and testing data: \n",
            "\n",
            "0.7850375, 0.7313221012509377, 0.9014144342263095, 0.8075084789397687, 0.7835, 0.7309263192288842, 0.8962339743589743, 0.8051831188697922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs3URC7_naKK"
      },
      "source": [
        "### SVM trained word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIbolcCTAr_l",
        "outputId": "b51f0f67-b201-44c0-fb6e-6c8634f03a5c"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "clfSVM = LinearSVC()\n",
        "clfSVM.fit(X_train_word2vec, y_train_word2vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-gaNC0fsVLH",
        "outputId": "9a0cf49f-567c-4038-d35e-c1489a7b45fa"
      },
      "source": [
        "print(\"SVM model report on training and testing data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec, clfSVM.predict(X_train_word2vec), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec, clfSVM.predict(X_test_word2vec), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM model report on training and testing data: \n",
            "\n",
            "0.87375, 0.8720155192996418, 0.8761995201919233, 0.8741025129637016, 0.87155, 0.8685021369645165, 0.8752003205128205, 0.8718383636817161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1L2VuG6oB0Q"
      },
      "source": [
        "### SVM TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3w4wEAasc3c",
        "outputId": "265fb248-79ae-4a9a-8c7f-f06f2e0078cd"
      },
      "source": [
        "clfSVM = LinearSVC()\n",
        "clfSVM.fit(X_train_tfidf, y_train_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1MVc5-GtY2r",
        "outputId": "c07f7809-f69c-4377-aa55-ca0e301aba3d"
      },
      "source": [
        "print(\"SVM model report on training and testing data: \\n\")\n",
        "training_report = classification_report(y_train_tfidf, clfSVM.predict(X_train_tfidf), output_dict=True)\n",
        "testing_report = classification_report(y_test_tfidf, clfSVM.predict(X_test_tfidf), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM model report on training and testing data: \n",
            "\n",
            "0.947, 0.9467978819062843, 0.9472710915633746, 0.9470344276220456, 0.89295, 0.8941602171072469, 0.8910256410256411, 0.8925901770932624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_9cWuN_oEaE"
      },
      "source": [
        "### SVM my Google word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnbI-D_Q3gxz",
        "outputId": "5af18969-24b1-420f-c0f6-d74e0f4cf960"
      },
      "source": [
        "clfSVM = LinearSVC()\n",
        "clfSVM.fit(X_train_google_word2vec, y_train_google_word2vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd5BFmLJ6Pou",
        "outputId": "c23ba38a-0432-40c3-e408-dcdea451a7ab"
      },
      "source": [
        "print(\"SVM model report on training and testing data: \\n\")\n",
        "training_report = classification_report(y_train_google_word2vec, clfSVM.predict(X_train_google_word2vec), output_dict=True)\n",
        "testing_report = classification_report(y_test_google_word2vec, clfSVM.predict(X_test_google_word2vec), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM model report on training and testing data: \n",
            "\n",
            "0.83935, 0.8398558702832549, 0.838764494202319, 0.8393098274568641, 0.83495, 0.8366072327994358, 0.8318309294871795, 0.8342122444879715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbtBgtnuNCx"
      },
      "source": [
        "# Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIXC63vOvQMl"
      },
      "source": [
        "### Neural Network trained Word2vec on binary data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCPbaltAuOlO"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,10), activation='relu' ,random_state=1, max_iter=10000).fit(X_train_word2vec, y_train_word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1HmR40wF6-",
        "outputId": "6549dc9f-3719-470b-f10a-26a1020fe108"
      },
      "source": [
        "print(\"MLP model report on training and testing on binary data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec, mlp.predict(X_train_word2vec), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec, mlp.predict(X_test_word2vec), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP model report on training and testing binary data: \n",
            "\n",
            "0.9869375, 0.9857409946404089, 0.9881797281087565, 0.9869588548751419, 0.83575, 0.8342480790340285, 0.8373397435897436, 0.8357910522369408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYiJOPSY0etN"
      },
      "source": [
        "### Neural Network trained Word2vec on trinary data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4pqJHrR0pCK"
      },
      "source": [
        "# Create our X dataset\n",
        "X_word2vec_trinary = np.array(reviews['word2vec'].values.tolist())\n",
        "y_word2vec_trinary = reviews['trinary_rate']\n",
        "X_train_word2vec_trinary, X_test_word2vec_trinary, y_train_word2vec_trinary, y_test_word2vec_trinary = train_test_split(X_word2vec_trinary, y_word2vec_trinary, test_size=0.2, random_state=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8o6k1-_0o6g",
        "outputId": "5d30e6a0-fc9a-4bda-8ca2-2b7e462f1dc4"
      },
      "source": [
        "parameters = {\n",
        "    'hidden_layer_sizes': [(50,10)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05, 0.001],\n",
        "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
        "    'random_state': [1,2,3],\n",
        "    'max_iter': [300]\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(MLPClassifier(), parameters, verbose=2, n_jobs=-1)\n",
        "\n",
        "clf.fit(X_train_word2vec_trinary, y_train_word2vec_trinary)\n",
        "print(clf.score(X_train_word2vec_trinary, y_train_word2vec_trinary))\n",
        "print(clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 16.9min\n",
            "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 63.0min\n",
            "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 136.1min\n",
            "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 197.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7074333333333334\n",
            "{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 10), 'learning_rate': 'constant', 'max_iter': 300, 'random_state': 2, 'solver': 'sgd'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh4LRnHa7d7X"
      },
      "source": [
        "mlp_trinary = MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes= (50, 10), learning_rate= 'constant', max_iter=1000, random_state= 2, solver= 'sgd', verbose=True).fit(X_train_word2vec_trinary, y_train_word2vec_trinary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C5DWQf75-z3",
        "outputId": "22ee8084-5b1d-48a5-c328-8f1f8f344d40"
      },
      "source": [
        "print(\"MLP model report on training and testing on trinary data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec_trinary, mlp_trinary.predict(X_train_word2vec_trinary), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec_trinary, mlp_trinary.predict(X_test_word2vec_trinary), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model report on training and testing on trinary data: \n",
            "\n",
            "0.8382333333333334, 0.7975259039966166, 0.7508461079036433, 0.7734823625922886, 0.6365, 0.5424452749599573, 0.5139099645928173, 0.5277922077922077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTMZxUkrzyaF"
      },
      "source": [
        "### Neural Network pre-trained google Word2vec on binary data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwwvBPMC22nX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7f448b-b405-444f-f21e-d0a1d98b1f40"
      },
      "source": [
        "# TODO MLP training\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,10), activation='relu' ,random_state=1, max_iter=300).fit(X_train_google_word2vec, y_train_google_word2vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJqgXAqO6nN9",
        "outputId": "3ff546a0-92d2-4b70-e426-f99eb567dd41"
      },
      "source": [
        "print(\"MLP model report on training and testing on binary data: \\n\")\n",
        "training_report = classification_report(y_train_google_word2vec, mlp.predict(X_train_google_word2vec), output_dict=True)\n",
        "testing_report = classification_report(y_test_google_word2vec, mlp.predict(X_test_google_word2vec), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model report on training and testing on binary data: \n",
            "\n",
            "0.914075, 0.9116609529487752, 0.9170831667333067, 0.9143640214276816, 0.8368, 0.8368758772809304, 0.8360376602564102, 0.8364565587734241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiCsdv6x21Lx"
      },
      "source": [
        "### Neural Network pre-trained google Word2vec on trianry data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8nMqxfM2rZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c500b80f-613e-453d-bdec-e2916ad449d8"
      },
      "source": [
        "X_word2vec_google_trinary = np.array(reviews['google_word2vec'].values.tolist())\n",
        "y_word2vec_google_trinary = reviews['trinary_rate']\n",
        "X_train_word2vec_google_trinary, X_test_word2vec_google_trinary, y_train_word2vec_google_trinary, y_test_word2vec_google_trinary = train_test_split(X_word2vec_google_trinary, y_word2vec_google_trinary, test_size=0.2, random_state=200)\n",
        "mlp_trinary = MLPClassifier(hidden_layer_sizes=(50,10), activation='relu' ,random_state=1, max_iter=300).fit(X_train_word2vec_google_trinary, y_train_word2vec_google_trinary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZvhoBRsCCSB",
        "outputId": "988227ee-3cab-45bf-b625-deca0cbc608f"
      },
      "source": [
        "print(\"MLP model report on training and testing on binary data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec_google_trinary, mlp_trinary.predict(X_train_word2vec_google_trinary), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec_google_trinary, mlp_trinary.predict(X_test_word2vec_google_trinary), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model report on training and testing on binary data: \n",
            "\n",
            "0.8157666666666666, 0.750701907251428, 0.7718494923352578, 0.7611288343558281, 0.5903333333333334, 0.48189280540801543, 0.5048052604957005, 0.49308300395256915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oiFrP-cJf85"
      },
      "source": [
        "## Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wYTR6tQ2-n6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7596496-71fd-44d7-c0a3-7061a0255064"
      },
      "source": [
        "# Trained only first 10 words\n",
        "import numpy as np\n",
        "\n",
        "indexs = reviews.index\n",
        "reviews['word2vec_10'] = pd.Series(dtype=object)\n",
        "for idx , review in zip(indexs, reviews['clean_body']):\n",
        "  count_words = 0\n",
        "  unseen_words = 0\n",
        "  x = 0\n",
        "  for word in review.split():\n",
        "    try:\n",
        "      x = x + w2v_model[word]\n",
        "      count_words = count_words + 1\n",
        "    except KeyError:\n",
        "      unseen_words = unseen_words + 1\n",
        "      # reviews.at[idx, 'word2vec_10'] = np.NaN\n",
        "      continue\n",
        "  \n",
        "  x = x/(10)\n",
        "  x1 = x.reshape(-1, 1)\n",
        "  x1 = x1.T\n",
        "  reviews.at[idx, 'word2vec_10'] = x1[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oj0pUzwFGUv"
      },
      "source": [
        "binary_reviews_10 = reviews[['clean_body','word2vec_10', 'binary_rate']].dropna()\n",
        "# Create our X dataset\n",
        "X_word2vec_10 = np.array(binary_reviews_10['word2vec_10'].values.tolist())\n",
        "y_word2vec_10 = binary_reviews['binary_rate']\n",
        "X_train_word2vec_10, X_test_word2vec_10, y_train_word2vec_10, y_test_word2vec_10 = train_test_split(X_word2vec_10, y_word2vec_10, test_size=0.2, random_state=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkvee4VxFhZ_"
      },
      "source": [
        "mlp_10 = MLPClassifier(hidden_layer_sizes=(50,10), activation='relu' ,random_state=1, max_iter=300).fit(X_train_word2vec_10, y_train_word2vec_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHbsKyMfFulx",
        "outputId": "a163ee5d-0310-4005-dc4f-7bb0b37ab25f"
      },
      "source": [
        "print(\"MLP model report on training and testing on binary data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec_10, mlp_10.predict(X_train_word2vec_10), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec_10, mlp_10.predict(X_test_word2vec_10), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model report on training and testing on binary data: \n",
            "\n",
            "0.9695625, 0.9736344617245986, 0.9652888844462215, 0.9694437124320797, 0.8408, 0.8441992306134846, 0.8352363782051282, 0.8396938878259995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epW9o2CWHPnN"
      },
      "source": [
        "### Trinary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying amazon reviews into Positive, Neutral, Negative. Achieved a 58 f1 score"
      ],
      "metadata": {
        "id": "cjCovBOOqMRT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPQDkCeXFxRc"
      },
      "source": [
        "X_word2vec_trinary_10 = np.array(reviews['word2vec'].values.tolist())\n",
        "y_word2vec_trinary_10 = reviews['trinary_rate']\n",
        "X_train_word2vec_trinary_10, X_test_word2vec_trinary_10, y_train_word2vec_trinary_10, y_test_word2vec_trinary_10 = train_test_split(X_word2vec_trinary_10, y_word2vec_trinary_10, test_size=0.2, random_state=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVec3TtCHGt3"
      },
      "source": [
        "mlp_trinary_10 = MLPClassifier(solver='lbfgs', activation='tanh', alpha=0.05, hidden_layer_sizes= (50, 10), learning_rate= 'constant', max_iter=300, random_state= 2, solver= 'sgd', verbose=True).fit(X_train_word2vec_trinary_10, y_train_word2vec_trinary_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1ameKdtHPRn",
        "outputId": "6d99268c-09b3-4189-e4a7-f67552f4395b"
      },
      "source": [
        "print(\"MLP model report on training and testing on binary data: \\n\")\n",
        "training_report = classification_report(y_train_word2vec_trinary_10, mlp_trinary_10.predict(X_train_word2vec_trinary_10), output_dict=True)\n",
        "testing_report = classification_report(y_test_word2vec_trinary_10, mlp_trinary_10.predict(X_test_word2vec_trinary_10), output_dict=True)\n",
        "report_print(training_report, testing_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP model report on training and testing on binary data: \n",
            "\n",
            "0.709525, 0.6116304509969668, 0.6146562062762343, 0.61313959574309, 0.6840666666666667, 0.5846339744235223, 0.5819967923015237, 0.5833124026724268\n"
          ]
        }
      ]
    }
  ]
}